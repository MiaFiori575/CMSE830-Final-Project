{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b840b0b-45fe-495c-abbe-81268110a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2beedff7-7bf7-4529-b9b6-8dd5c80f638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FinalProj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FinalProj.py\n",
    "#loading data into the data frame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import openpyxl\n",
    "from scipy.signal import find_peaks\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "\n",
    "# Loading data sheets according to sample type\n",
    "treated_waves= pd.read_excel(\"Sample no Filter.xlsx\", sheet_name=\"Samples\", header=None)\n",
    "clean_waves= pd.read_excel(\"Sample no Filter.xlsx\", sheet_name=\"Clean Water\", header=None)\n",
    "bw_waves= pd.read_excel(\"Sample no Filter.xlsx\", sheet_name=\"Blackwater\", header=None)\n",
    "\n",
    "#grouping triplicate measurements and taking the average, then replacing the triplicate measurements with the average in the\n",
    "#data frame\n",
    "\n",
    "##blackwater\n",
    "# Assume first column = sample names, rest = numeric data\n",
    "bw_waves.columns = [\"Sample\"] + [f\"Col{i}\" for i in range(1, bw_waves.shape[1])]\n",
    "# Extract group prefix (BW1, BW2, BW3...)\n",
    "bw_waves[\"Group\"] = bw_waves[\"Sample\"].str.extract(r'^(BW\\d+)')\n",
    "# Compute averages across all numeric columns per group\n",
    "bw_waves = bw_waves.groupby(\"Group\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "##treated_water\n",
    "treated_waves = treated_waves.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Rename columns: first col = Sample, rest = numeric data\n",
    "treated_waves.columns = [\"Sample\"] + [f\"Col{i}\" for i in range(1, treated_waves.shape[1])]\n",
    "\n",
    "# Extract group prefix: everything before the first dash\n",
    "treated_waves[\"Group\"] = treated_waves[\"Sample\"].astype(str).str.extract(r'^([A-Za-z0-9]+)')\n",
    "\n",
    "# Compute averages across all numeric columns per group\n",
    "treated_waves = treated_waves.groupby(\"Group\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "##clean_water\n",
    "clean_waves = clean_waves.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Rename columns: first col = Sample, rest = numeric data\n",
    "clean_waves.columns = [\"Sample\"] + [f\"Col{i}\" for i in range(1, clean_waves.shape[1])]\n",
    "\n",
    "# Extract group prefix: everything before the first dash\n",
    "clean_waves[\"Group\"] = clean_waves[\"Sample\"].astype(str).str.extract(r'^([A-Za-z0-9]+)')\n",
    "\n",
    "# Compute averages across all numeric columns per group\n",
    "clean_waves = clean_waves.groupby(\"Group\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "#merging related data\n",
    "tw_bw_clean_waves = pd.concat([treated_waves, bw_waves, clean_waves])\n",
    "\n",
    "\n",
    "#importing turbidity measurements \n",
    "turb = pd.read_excel('physiochemical data.xlsx', sheet_name='Master Physiochem', usecols=['Turbidity (NTU)'])\n",
    "#checking that data was correctly imported\n",
    "\n",
    "#importing total suspended solids measurements \n",
    "tss = pd.read_excel('physiochemical data.xlsx', sheet_name='TSS', usecols=['TSS (% w/v)'])\n",
    "\n",
    "#importing total solids measurements \n",
    "ts = pd.read_excel('physiochemical data.xlsx', sheet_name='TS', usecols=['%TS (w/w)'])\n",
    "\n",
    "#missing and not missing\n",
    "tss_missing = tss[tss.isnull().any(axis=1)]\n",
    "tss_not_missing = tss.dropna()\n",
    "\n",
    "#preparing scalar for KNN\n",
    "scaler = StandardScaler()\n",
    "tss_scaled = pd.DataFrame(scaler.fit_transform(tss_not_missing), columns = tss_not_missing.columns)\n",
    "\n",
    "#intialize and fit KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights ='distance')\n",
    "#here we have to make the scatter plot of the data without missing values so they aren't skewed by missing values, then we overlay the missing values \n",
    "#on the scatter plot \n",
    "#what does .fit do? Training on non-missing data ---it's machine learning and has never seen this data set before, so it has to train\n",
    "imputer.fit(tss_scaled)\n",
    "\n",
    "# function to impute and inverse transform the data\n",
    "def impute_and_inverse_transform(data):\n",
    "    # Ensure 'data' is always a DataFrame with proper column names\n",
    "    scaled_data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "    imputed_scaled = imputer.transform(scaled_data)\n",
    "    return pd.DataFrame(scaler.inverse_transform(imputed_scaled), columns=data.columns, index=data.index)\n",
    "\n",
    "# impute missing values\n",
    "tss_imputed = impute_and_inverse_transform(tss)\n",
    "\n",
    "#missing and not missing\n",
    "turb_missing = turb[turb.isnull().any(axis=1)]\n",
    "turb_not_missing = turb.dropna()\n",
    "\n",
    "#preparing scalar for KNN\n",
    "scaler = StandardScaler()\n",
    "turb_scaled = pd.DataFrame(scaler.fit_transform(turb_not_missing), columns = turb_not_missing.columns)\n",
    "\n",
    "#intialize and fit KNN imputer\n",
    "#imputer = KNNImputer(n_neighbors=5)\n",
    "#modded above function to include 10 neighbors and weight the imputed value depending on how close the points were-the closer the point the more it \n",
    "#affects the imputing value\n",
    "imputer = KNNImputer(n_neighbors=3, weights ='distance')\n",
    "#here we have to make the scatter plot of the data without missing values so they aren't skewed by missing values, then we overlay the missing values \n",
    "#on the scatter plot \n",
    "#what does .fit do? Training on non-missing data ---it's machine learning and has never seen this data set before, so it has to train\n",
    "imputer.fit(turb_scaled)\n",
    "# impute missing values\n",
    "turb_imputed = impute_and_inverse_transform(turb)\n",
    "\n",
    "#tss and turb data was imputed, no data missing for ts\n",
    "\n",
    "tw_tss = tss_imputed[0:96]\n",
    "tw_ts = ts[0:96]\n",
    "tw_turb = turb_imputed[0:96]\n",
    "\n",
    "bw_tss = tss_imputed[96:106]\n",
    "bw_ts = ts[96:106]\n",
    "bw_turb = turb_imputed[96:106]\n",
    "\n",
    "#reading sample names\n",
    "sample_IDs = pd.read_excel('physiochemical data.xlsx', sheet_name='Master Physiochem', usecols=['Sample'])\n",
    "\n",
    "#merging related data\n",
    "tw_physio = pd.concat([tw_tss, tw_ts,tw_turb], axis=1)\n",
    "bw_physio = pd.concat([bw_tss, bw_ts,bw_turb], axis=1)\n",
    "tw_bw_physio = pd.concat([ tw_physio,bw_physio])\n",
    "\n",
    "# Add the Sample IDs as a new column\n",
    "tw_bw_physio.insert(0, 'Sample', sample_IDs['Sample'].values)\n",
    "\n",
    "# Define column names (excluding 'Sample')\n",
    "value_cols = [col for col in tw_bw_physio.columns if col != 'Sample']\n",
    "\n",
    "# Generate random values with different ranges\n",
    "col1 = np.random.uniform(0, 0.00002, size=(7,))   # TSS (% w/v)\n",
    "col2 = np.random.uniform(0, 0.00002, size=(7,))   # %TS (w/w)\n",
    "col3 = np.random.uniform(0, 0.02, size=(7,))     # Turbidity (NTU)\n",
    "\n",
    "# Stack into DataFrame\n",
    "new_df = pd.DataFrame(\n",
    "    np.column_stack([col1, col2, col3]),\n",
    "    columns=value_cols\n",
    ")\n",
    "\n",
    "# Add the Sample IDs as the first column\n",
    "new_df = pd.concat(\n",
    "    [clean_waves[['Group']].rename(columns={'Group':'Sample'}).head(7), new_df],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Append to the original DataFrame\n",
    "tw_bw_clean_physio = pd.concat([tw_bw_physio, new_df], ignore_index=True)\n",
    "\n",
    "# Assume your DataFrame is called df with columns:\n",
    "# ['Sample', 'TSS (% w/v)', '%TS (w/w)', 'Turbidity (NTU)']\n",
    "\n",
    "# Segregating samples according to sample name [treated water (A1, B2, etc), clean (Blank1, Blank2), blackwater feed (BW1, BW2) etc)\n",
    "def classify_sample(sample):\n",
    "    if sample.startswith(\"Blank\"):\n",
    "        return \"Clean Water\"\n",
    "    elif sample.startswith(\"BW\"):\n",
    "        return \"Contaminated Water\"\n",
    "    else:\n",
    "        return \"Treated Water\"\n",
    "\n",
    "# Use classify sample\n",
    "tw_bw_clean_physio['Type'] = tw_bw_clean_physio['Sample'].apply(classify_sample)\n",
    "\n",
    "# Describe statistics in a separate value\n",
    "stats = tw_bw_clean_physio.groupby('Type').describe()\n",
    "\n",
    "\n",
    "grouped_stats = tw_bw_clean_physio.groupby('Type')[['TSS (% w/v)', '%TS (w/w)', 'Turbidity (NTU)']].agg(['mean','std'])\n",
    "\n",
    "# Extract mean and std separately for plotting\n",
    "means = grouped_stats.xs('mean', axis=1, level=1)\n",
    "stds  = grouped_stats.xs('std', axis=1, level=1)\n",
    "\n",
    "#tw_bw_clean_waves['Type'] = tw_bw_clean_waves['Sample'].apply(classify_sample)\n",
    "wavelengths = pd.read_excel(\"Sample no Filter.xlsx\", sheet_name=\"Samples\", nrows=1, header = None)\n",
    "wavelengths = wavelengths.drop(columns=[0])\n",
    "\n",
    "tw_bw_clean_waves['Type'] = tw_bw_clean_waves['Group'].apply(classify_sample)\n",
    "\n",
    "# --- Step 2: Extract wavelength values ---\n",
    "wavelengths_values = wavelengths.values.flatten()  # drop first col if needed\n",
    "\n",
    "# --- Step 1: Compute mean spectra and find maxima/minima indices ---\n",
    "mean_spectra = {}\n",
    "for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "    data = group.drop(columns=['Group','Type']).values\n",
    "    mean_spectra[sample_type] = data.mean(axis=0)\n",
    "\n",
    "# Find maxima/minima for each type\n",
    "maxima_sets = []\n",
    "minima_sets = []\n",
    "for mean in mean_spectra.values():\n",
    "    maxima_idx, _ = find_peaks(mean)\n",
    "    minima_idx, _ = find_peaks(-mean)\n",
    "    maxima_sets.append(maxima_idx)\n",
    "    minima_sets.append(minima_idx)\n",
    "\n",
    "# Function to find common peaks within tolerance\n",
    "def find_common_peaks(sets, tolerance=5):\n",
    "    common = []\n",
    "    ref = sets[0]\n",
    "    for idx in ref:\n",
    "        if all(any(abs(idx - other) <= tolerance for other in s) for s in sets[1:]):\n",
    "            common.append(idx)\n",
    "    return np.array(common)\n",
    "common_maxima = find_common_peaks(maxima_sets, tolerance=5)\n",
    "common_minima = find_common_peaks(minima_sets, tolerance=5)\n",
    "\n",
    "# --- Step 2: Extract wavelengths at these indices ---\n",
    "selected_indices = np.sort(np.concatenate([common_maxima, common_minima]))\n",
    "selected_wavelengths = wavelengths.iloc[0, selected_indices].values\n",
    "\n",
    "# --- Step 3: Build new DataFrame ---\n",
    "rows = []\n",
    "# First row: wavelengths\n",
    "header_row = [\"Sample_ID\"] + selected_wavelengths.tolist()\n",
    "rows.append(header_row)\n",
    "\n",
    "# Subsequent rows: sample ID + values at selected indices\n",
    "for _, row in tw_bw_clean_waves.iterrows():\n",
    "    sample_id = row['Group']\n",
    "    values = row.drop(labels=['Group','Type']).values[selected_indices]\n",
    "    rows.append([sample_id] + values.tolist())\n",
    "\n",
    "# Convert to DataFrame\n",
    "compeak_sum = pd.DataFrame(rows)\n",
    "\n",
    "# --- Step 4: Optional formatting ---\n",
    "compeak_sum.columns = [\"Sample_ID\"] + [f\"Wavelength_{w}\" for w in selected_wavelengths]\n",
    "# Compute summary statistics for each sample type\n",
    "summary_stats = []\n",
    "\n",
    "for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "    data = group.drop(columns=['Group','Type']).values[:, selected_indices]\n",
    "    \n",
    "    stats = {\n",
    "        \"Sample_Type\": sample_type,\n",
    "        \"N_samples\": data.shape[0]\n",
    "    }\n",
    "    \n",
    "    # For each wavelength, compute stats\n",
    "    for i, wl in enumerate(selected_wavelengths):\n",
    "        stats[f\"{wl}_mean\"]   = data[:, i].mean()\n",
    "        stats[f\"{wl}_std\"]    = data[:, i].std()\n",
    "        stats[f\"{wl}_min\"]    = data[:, i].min()\n",
    "        stats[f\"{wl}_max\"]    = data[:, i].max()\n",
    "        stats[f\"{wl}_median\"] = np.median(data[:, i])\n",
    "    \n",
    "    summary_stats.append(stats)\n",
    "\n",
    "# --- Step 4: Convert to DataFrame ---\n",
    "compeak_summary = pd.DataFrame(summary_stats)\n",
    "# Ensure both DataFrames have Sample_ID as the first column\n",
    "if compeak_sum.columns[0] != \"Sample_ID\":\n",
    "    compeak_sum = compeak_sum.rename(columns={compeak_sum.columns[0]: \"Sample_ID\"})\n",
    "if tw_bw_clean_physio.columns[0] != \"Sample_ID\":\n",
    "    tw_bw_clean_physio = tw_bw_clean_physio.rename(columns={tw_bw_clean_physio.columns[0]: \"Sample_ID\"})\n",
    "\n",
    "# --- Step 1: Merge on Sample_ID ---\n",
    "merged_df = pd.merge(\n",
    "    tw_bw_clean_physio,\n",
    "    compeak_sum,\n",
    "    on=\"Sample_ID\",\n",
    "    how=\"inner\"   # only keep matching IDs\n",
    ")\n",
    "\n",
    "# --- Step 2: Reorder columns so tw_bw_clean_physio comes first ---\n",
    "ordered_cols = [\"Sample_ID\"] + \\\n",
    "               [col for col in tw_bw_clean_physio.columns if col != \"Sample_ID\"] + \\\n",
    "               [col for col in compeak_sum.columns if col != \"Sample_ID\"]\n",
    "\n",
    "waves_physio = merged_df[ordered_cols]\n",
    "\n",
    "# --- Step 1: Ensure Sample_ID alignment ---\n",
    "if compeak_sum.columns[0] != \"Sample_ID\":\n",
    "    compeak_sum = compeak_sum.rename(columns={compeak_sum.columns[0]: \"Sample_ID\"})\n",
    "if tw_bw_clean_physio.columns[0] != \"Sample_ID\":\n",
    "    tw_bw_clean_physio = tw_bw_clean_physio.rename(columns={tw_bw_clean_physio.columns[0]: \"Sample_ID\"})\n",
    "\n",
    "# --- Step 2: Merge absorbance + physiochemical data ---\n",
    "# TSS (% w/v)     %TS (w/w)  Turbidity (NTU) \n",
    "merged_df = pd.merge(\n",
    "    tw_bw_clean_physio[[\"Sample_ID\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"]],\n",
    "    compeak_sum,\n",
    "    on=\"Sample_ID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# --- Step 3: Compute correlation matrix ---\n",
    "corr_matrix = merged_df.drop(columns=[\"Sample_ID\"]).corr()\n",
    "\n",
    "# --- Step 4: Extract correlations of TS, TSS, Turbidity vs wavelengths ---\n",
    "physio_corr = corr_matrix.loc[[\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"], compeak_sum.columns[1:]]\n",
    "# --- Step 1: Keep only numeric columns ---\n",
    "# Drop Sample_ID and any non-numeric columns like 'Type'\n",
    "#X = waves_physio.select_dtypes(include=[np.number])\n",
    "\n",
    "# --- Step 2: Standardize data ---\n",
    "#X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# --- Step 3: Run PCA ---\n",
    "#pca = PCA()\n",
    "#X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# --- Step 3: Create DataFrame with PCA results + sample type ---\n",
    "#pca_df = pd.DataFrame({\n",
    "#    \"PC1\": X_pca[:,0],\n",
    "#    \"PC2\": X_pca[:,1],\n",
    "#    \"Type\": waves_physio[\"Type\"].values\n",
    "#})\n",
    "\n",
    "# Drop Sample_ID and any non-numeric columns like 'Type'\n",
    "X = waves_physio.select_dtypes(include=[np.number])\n",
    "\n",
    "# --- Step 2: Standardize data ---\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# --- Step 3: Run PCA ---\n",
    "pca = PCA(n_components=8)   # explicitly set number of components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# --- Step 4: Create DataFrame with PCA results + sample type ---\n",
    "# Keep ALL components for scree plot, but we‚Äôll only use PC1/PC2 for biplot\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "pca_df[\"Type\"] = waves_physio[\"Type\"].values\n",
    "\n",
    "# --- Step 1: Keep only numeric predictors (drop Sample_ID and Type) ---\n",
    "X = waves_physio.drop(columns=[\"Sample_ID\",\"Type\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"])\n",
    "y_tss = waves_physio[\"TSS (% w/v)\"]\n",
    "y_turb = waves_physio[\"Turbidity (NTU)\"]\n",
    "\n",
    "# --- Step 2: Standardize predictors ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- Step 3: Fit regression models ---\n",
    "model_tss = LinearRegression().fit(X_scaled, y_tss)\n",
    "model_turb = LinearRegression().fit(X_scaled, y_turb)\n",
    "\n",
    "# --- Step 4: Output regression functions ---\n",
    "print(\"TSS model intercept:\", model_tss.intercept_)\n",
    "print(\"TSS model coefficients:\", dict(zip(X.columns, model_tss.coef_)))\n",
    "\n",
    "print(\"\\nTurbidity model intercept:\", model_turb.intercept_)\n",
    "print(\"Turbidity model coefficients:\", dict(zip(X.columns, model_turb.coef_)))\n",
    "\n",
    "# --- Step 5: Evaluate performance ---\n",
    "y_tss_pred = model_tss.predict(X_scaled)\n",
    "y_turb_pred = model_turb.predict(X_scaled)\n",
    "\n",
    "# Predictions from your fitted models\n",
    "y_tss_pred = model_tss.predict(X_scaled)\n",
    "y_turb_pred = model_turb.predict(X_scaled)\n",
    "\n",
    "# Metrics\n",
    "tss_r2 = r2_score(y_tss, y_tss_pred)\n",
    "tss_mse = mean_squared_error(y_tss, y_tss_pred)\n",
    "tss_rmse = np.sqrt(tss_mse)\n",
    "\n",
    "turb_r2 = r2_score(y_turb, y_turb_pred)\n",
    "turb_mse = mean_squared_error(y_turb, y_turb_pred)\n",
    "turb_rmse = np.sqrt(turb_mse)\n",
    "\n",
    "# --- Step 1: Prepare predictors and targets ---\n",
    "X = waves_physio.drop(columns=[\"Sample_ID\",\"Type\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"])\n",
    "y_tss = waves_physio[\"TSS (% w/v)\"]\n",
    "y_turb = waves_physio[\"Turbidity (NTU)\"]\n",
    "\n",
    "# --- Step 2: Standardize predictors ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- Step 3: Fit regression models ---\n",
    "model_tss = LinearRegression().fit(X_scaled, y_tss)\n",
    "model_turb = LinearRegression().fit(X_scaled, y_turb)\n",
    "\n",
    "# --- Step 4: Function to print equation ---\n",
    "def print_equation(model, X, target_name):\n",
    "    intercept = model.intercept_\n",
    "    coefs = model.coef_\n",
    "    terms = [f\"{coef:.3f}*{feature}\" for coef, feature in zip(coefs, X.columns)]\n",
    "    equation = f\"{target_name} = {intercept:.3f} + \" + \" + \".join(terms)\n",
    "    return equation\n",
    "\n",
    "\n",
    "# Print equations\n",
    "#print_equation(model_tss, X, \"TSS (% w/v)\")\n",
    "#print_equation(model_turb, X, \"Turbidity (NTU)\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Step 5: Build comparison DataFrame ---\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Wavelength\": X.columns,\n",
    "    \"TSS_Coefficient\": model_tss.coef_,\n",
    "    \"Turbidity_Coefficient\": model_turb.coef_\n",
    "})\n",
    "\n",
    "# Add absolute values for easier ranking\n",
    "coef_df[\"|TSS|\"] = coef_df[\"TSS_Coefficient\"].abs()\n",
    "coef_df[\"|Turbidity|\"] = coef_df[\"Turbidity_Coefficient\"].abs()\n",
    "\n",
    "# Sort by strongest contributors for clarity\n",
    "coef_df_sorted = coef_df.sort_values(by=\"|TSS|\", ascending=False)\n",
    "\n",
    "print(\"\\nCoefficient Comparison Table:\")\n",
    "print(coef_df_sorted)\n",
    "\n",
    "# --- Step 5: Build comparison DataFrame ---\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Wavelength\": X.columns,\n",
    "    \"TSS_Coefficient\": model_tss.coef_,\n",
    "    \"Turbidity_Coefficient\": model_turb.coef_\n",
    "})\n",
    "\n",
    "# --- Step 1: Keep only numeric predictors ---\n",
    "X = waves_physio.drop(columns=[\"Sample_ID\",\"Type\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"])\n",
    "y_tss = waves_physio[\"TSS (% w/v)\"]\n",
    "y_turb = waves_physio[\"Turbidity (NTU)\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "# --- Step 2: Loop through each predictor individually ---\n",
    "for feature in X.columns:\n",
    "    X_single = X[[feature]]  # one variable at a time\n",
    "    \n",
    "    # Fit regression for TSS\n",
    "    model_tss = LinearRegression().fit(X_single, y_tss)\n",
    "    y_tss_pred = model_tss.predict(X_single)\n",
    "    tss_r2 = r2_score(y_tss, y_tss_pred)\n",
    "    tss_rmse = np.sqrt(mean_squared_error(y_tss, y_tss_pred))\n",
    "    \n",
    "    # Fit regression for Turbidity\n",
    "    model_turb = LinearRegression().fit(X_single, y_turb)\n",
    "    y_turb_pred = model_turb.predict(X_single)\n",
    "    turb_r2 = r2_score(y_turb, y_turb_pred)\n",
    "    turb_rmse = np.sqrt(mean_squared_error(y_turb, y_turb_pred))\n",
    "    \n",
    "    results.append({\n",
    "        \"Feature\": feature,\n",
    "        \"TSS R¬≤\": round(tss_r2, 3),\n",
    "        \"TSS RMSE\": round(tss_rmse, 3),\n",
    "        \"Turbidity R¬≤\": round(turb_r2, 3),\n",
    "        \"Turbidity RMSE\": round(turb_rmse, 3)\n",
    "    })\n",
    "\n",
    "# --- Step 3: Convert results to DataFrame for inspection ---\n",
    "decomp_results = pd.DataFrame(results)\n",
    "\n",
    "fig_tss = px.bar(decomp_results, x=\"Feature\", y=\"TSS R¬≤\", title=\"TSS R¬≤ by Wavelength\", color=\"TSS R¬≤\")\n",
    "fig_turb = px.bar(decomp_results, x=\"Feature\", y=\"Turbidity R¬≤\", title=\"Turbidity R¬≤ by Wavelength\", color=\"Turbidity R¬≤\")\n",
    "\n",
    "# Top wavelengths\n",
    "top_wavelengths = [\n",
    "    \"Wavelength_951.46\",\n",
    "    \"Wavelength_957.655\",\n",
    "    \"Wavelength_970.044\",\n",
    "    \"Wavelength_976.238\",\n",
    "    \"Wavelength_1186.846\"\n",
    "]\n",
    "\n",
    "X_top = waves_physio[top_wavelengths]\n",
    "\n",
    "# Full spectrum (drop non-numeric identifiers and targets)\n",
    "X_full = waves_physio.drop(columns=[\n",
    "    \"Sample_ID\",\"Type\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"\n",
    "])\n",
    "\n",
    "# Targets\n",
    "y_tss = waves_physio[\"TSS (% w/v)\"]\n",
    "y_turb = waves_physio[\"Turbidity (NTU)\"]\n",
    "\n",
    "#Scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_top_scaled = scaler.fit_transform(X_top)\n",
    "X_full_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "#Fitting Linear Regression Models\n",
    "# Top wavelengths\n",
    "model_tss_top = LinearRegression().fit(X_top_scaled, y_tss)\n",
    "model_turb_top = LinearRegression().fit(X_top_scaled, y_turb)\n",
    "\n",
    "# Full spectrum\n",
    "model_tss_full = LinearRegression().fit(X_full_scaled, y_tss)\n",
    "model_turb_full = LinearRegression().fit(X_full_scaled, y_turb)\n",
    "\n",
    "#evaluating fit\n",
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    return r2, rmse\n",
    "\n",
    "# Top wavelengths\n",
    "top_wavelengths = [\n",
    "    \"Wavelength_951.46\",\n",
    "    \"Wavelength_957.655\",\n",
    "    \"Wavelength_970.044\",\n",
    "    \"Wavelength_976.238\",\n",
    "    \"Wavelength_1186.846\"\n",
    "]\n",
    "\n",
    "X_top = waves_physio[top_wavelengths]\n",
    "\n",
    "# Full spectrum (drop non-numeric identifiers and targets)\n",
    "X_full = waves_physio.drop(columns=[\n",
    "    \"Sample_ID\",\"Type\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"\n",
    "])\n",
    "\n",
    "# Targets\n",
    "y_tss = waves_physio[\"TSS (% w/v)\"]\n",
    "y_turb = waves_physio[\"Turbidity (NTU)\"]\n",
    "\n",
    "#Scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_top_scaled = scaler.fit_transform(X_top)\n",
    "X_full_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "#Fitting Linear Regression Models\n",
    "# Top wavelengths\n",
    "model_tss_top = LinearRegression().fit(X_top_scaled, y_tss)\n",
    "model_turb_top = LinearRegression().fit(X_top_scaled, y_turb)\n",
    "\n",
    "# Full spectrum\n",
    "model_tss_full = LinearRegression().fit(X_full_scaled, y_tss)\n",
    "model_turb_full = LinearRegression().fit(X_full_scaled, y_turb)\n",
    "\n",
    "#evaluating fit\n",
    "def evaluate(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    return r2, rmse\n",
    "\n",
    "tss_r2_top, tss_rmse_top = evaluate(model_tss_top, X_top_scaled, y_tss)\n",
    "tss_r2_full, tss_rmse_full = evaluate(model_tss_full, X_full_scaled, y_tss)\n",
    "\n",
    "turb_r2_top, turb_rmse_top = evaluate(model_turb_top, X_top_scaled, y_turb)\n",
    "turb_r2_full, turb_rmse_full = evaluate(model_turb_full, X_full_scaled, y_turb)\n",
    "\n",
    "#visualizing coefficients\n",
    "# TSS coefficients\n",
    "coef_tss = dict(zip(top_wavelengths, model_tss_top.coef_))\n",
    "coef_turb = dict(zip(top_wavelengths, model_turb_top.coef_))\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Page 1: Project Overview\n",
    "# -------------------------\n",
    "def page1():\n",
    "    st.title(\"üåäüíßüß™ Absorbance ‚Üí TSS & Turbidity Project üö∞‚öóÔ∏è\")\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    ### üåç Why This Project?\n",
    "    Water quality is a critical issue worldwide üåä. Suspended solids and turbidity directly affect ecosystems üêü,\n",
    "    drinking water safety üö∞, and wastewater treatment efficiency üè≠. By using absorbance data at specific wavelengths,\n",
    "    we can build predictive models that help monitor and manage water quality more effectively üíß.\n",
    "\n",
    "    ### üî¨ Scientific Motivation\n",
    "    Spectroscopy provides a rapid, non-destructive way to analyze water samples üß™. Instead of relying solely on\n",
    "    traditional lab methods ‚öóÔ∏è, absorbance readings can be transformed into meaningful predictions of TSS and Turbidity.\n",
    "    This bridges environmental science üå± with data science üìä, creating scalable solutions for water monitoring.\n",
    "\n",
    "    ### üöÄ Personal Goal\n",
    "    My aim is to demonstrate how reproducible modeling workflows can connect raw spectral data üìà with real-world\n",
    "    water quality outcomes üåä. This project is both a scientific exploration üî¨ and a practical tool for\n",
    "    wastewater management üè≠ and clean water initiatives üíß.\n",
    "    \"\"\")\n",
    "\n",
    "# -------------------------\n",
    "# Page 2: IDA\n",
    "# -------------------------\n",
    "def page2():\n",
    "    st.title(\"IDA\")\n",
    "\n",
    "    option = st.selectbox(\n",
    "        \"Select what to view:\",\n",
    "        [\n",
    "            \"Option 1: Data collection and importation\",\n",
    "            \"Option 2: Data cleaning and preprocessing\",\n",
    "            \"Option 3: Basic descriptive statistics\",\n",
    "            \"Option 4: Missing data analysis\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if option == \"Option 1: Data collection and importation\":\n",
    "        st.subheader(\"üì• Data Collection and Importation\")\n",
    "        st.write(\"Placeholder text: Describe how absorbance data was collected.\")\n",
    "        st.dataframe(tw_bw_clean_waves.head())\n",
    "\n",
    "    elif option == \"Option 2: Data cleaning and preprocessing\":\n",
    "        st.subheader(\"üßπ Data Cleaning and Preprocessing\")\n",
    "        st.write(\"Placeholder text: Describe splitting between blackwater, treated water, and clean water.\")\n",
    "        st.dataframe(tw_bw_clean_physio.head())\n",
    "\n",
    "    elif option == \"Option 3: Basic descriptive statistics\":\n",
    "        st.subheader(\"üìä Basic Descriptive Statistics\")\n",
    "        st.dataframe(stats)\n",
    "\n",
    "    elif option == \"Option 4: Missing data analysis\":\n",
    "        st.subheader(\"‚ùì Missing Data Analysis\")\n",
    "        merged_turbtssts = pd.concat([turb, tss, ts], axis=1)\n",
    "        fig, ax = plt.subplots(figsize=(12,18))  # larger heatmap\n",
    "        sns.heatmap(merged_turbtssts.isna(), cmap=\"rocket\", ax=ax)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "# -------------------------\n",
    "# Page 3: EDA\n",
    "# -------------------------\n",
    "def page3():\n",
    "    st.title(\"EDA: Exploratory Data Analysis\")\n",
    "    st.markdown(\n",
    "    \"<h3 style='color:#C71585; font-weight:bold;'>Click the expand button in the top right corner of the figure for a zoomed in view! - üîç</h3>\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "\n",
    "    option = st.selectbox(\n",
    "        \"Select what to view:\",\n",
    "        [\n",
    "            \"Option 1: Visualizing means of physiochemical characteristics vs sample type\",\n",
    "            \"Option 2: Visualizing spectral fingerprints according to sample types\",\n",
    "            \"Option 3: Separate sample types and their absorbance plots-Interactive!\",\n",
    "            \"Option 4: Common peaks and valleys\",\n",
    "            \"Option 5: Summary statistics for common peaks and valleys\",\n",
    "            \"Option 6: Correlation? You decide\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if option == \"Option 1: Visualizing means of physiochemical characteristics vs sample type\":\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "        fig, ax = plt.subplots(figsize=(14,10))\n",
    "        means.plot(kind='bar', yerr=stds, capsize=4, color=colors, ax=ax)\n",
    "        ax.set_title(\"Mean values by Sample Type\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylabel(\"Mean Value (log)\")\n",
    "        ax.legend(title=\"Parameter\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    elif option == \"Option 2: Visualizing spectral fingerprints according to sample types\":\n",
    "        colors = {\"Clean Water\":\"blue\",\"Contaminated Water\":\"red\",\"Treated Water\":\"green\"}\n",
    "        fig, ax = plt.subplots(figsize=(16,12))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "            data = group.drop(columns=['Group','Type']).values\n",
    "            for row in data:\n",
    "                ax.plot(wavelengths_values, row, color=colors[sample_type], alpha=0.2)\n",
    "            mean = data.mean(axis=0); std = data.std(axis=0)\n",
    "            ax.plot(wavelengths_values, mean, color=colors[sample_type], linewidth=2, label=f\"{sample_type} mean\")\n",
    "            ax.fill_between(wavelengths_values, mean-std, mean+std, color=colors[sample_type], alpha=0.2)\n",
    "        ax.set_title(\"Spectra by Sample Type\")\n",
    "        ax.set_xlabel(\"Wavelength (nm)\"); ax.set_ylabel(\"Absorbance (AU)\")\n",
    "        ax.legend()\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    elif option == \"Option 3: Separate sample types and their absorbance plots-Interactive!\":\n",
    "        colors = {\"Clean Water\":\"blue\",\"Contaminated Water\":\"red\",\"Treated Water\":\"green\"}\n",
    "        traces, trace_groups = [], {}\n",
    "        for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "            data = group.drop(columns=['Group','Type']).values\n",
    "            mean = data.mean(axis=0); std = data.std(axis=0)\n",
    "            trace_groups[sample_type] = []\n",
    "            for row in data:\n",
    "                idx = len(traces)\n",
    "                traces.append(go.Scatter(x=wavelengths_values,y=row,mode='lines',\n",
    "                                         line=dict(color=colors[sample_type],width=1),\n",
    "                                         opacity=0.2,visible=(sample_type==\"Clean Water\"),\n",
    "                                         name=f\"{sample_type} sample\"))\n",
    "                trace_groups[sample_type].append(idx)\n",
    "            idx = len(traces)\n",
    "            traces.append(go.Scatter(x=wavelengths_values,y=mean,mode='lines',\n",
    "                                     line=dict(color=colors[sample_type],width=3),\n",
    "                                     visible=(sample_type==\"Clean Water\"),\n",
    "                                     name=f\"{sample_type} mean\"))\n",
    "            trace_groups[sample_type].append(idx)\n",
    "        buttons = []\n",
    "        for sample_type in trace_groups:\n",
    "            visible_mask = [False]*len(traces)\n",
    "            for idx in trace_groups[sample_type]: visible_mask[idx]=True\n",
    "            buttons.append(dict(label=sample_type,method=\"update\",\n",
    "                                args=[{\"visible\":visible_mask},{\"title\":f\"{sample_type} Spectra\"}]))\n",
    "        fig = go.Figure(data=traces)\n",
    "        fig.update_layout(title=\"Spectra by Sample Type\",xaxis_title=\"Wavelength (nm)\",\n",
    "                          yaxis_title=\"Absorbance (AU)\",template=\"plotly_white\",\n",
    "                          height=900,width=1200,updatemenus=[dict(active=0,buttons=buttons,x=1,y=1.15,\n",
    "                          xanchor=\"right\",yanchor=\"top\")])\n",
    "        st.plotly_chart(fig)\n",
    "\n",
    "    elif option == \"Option 4: Common peaks and valleys\":\n",
    "        st.write(\"Placeholder text: Why identifying common peaks and valleys is important.\")\n",
    "        colors = {\"Clean Water\":\"blue\",\"Contaminated Water\":\"red\",\"Treated Water\":\"green\"}\n",
    "        mean_spectra,std_spectra={},{}\n",
    "        for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "            data = group.drop(columns=['Group','Type']).values\n",
    "            mean_spectra[sample_type]=data.mean(axis=0); std_spectra[sample_type]=data.std(axis=0)\n",
    "        maxima_sets,minima_sets=[],[]\n",
    "        for sample_type, mean in mean_spectra.items():\n",
    "            maxima_idx,_=find_peaks(mean); minima_idx,_=find_peaks(-mean)\n",
    "            maxima_sets.append(set(maxima_idx)); minima_sets.append(set(minima_idx))\n",
    "        def find_common_peaks(sets,tolerance=5):\n",
    "            common=[]; ref=sets[0]\n",
    "            for idx in ref:\n",
    "                if all(any(abs(idx-other)<=tolerance for other in s) for s in sets[1:]): common.append(idx)\n",
    "            return np.array(common)\n",
    "        common_maxima=find_common_peaks(maxima_sets); common_minima=find_common_peaks(minima_sets)\n",
    "        fig,ax=plt.subplots(figsize=(16,12))\n",
    "        for sample_type in mean_spectra:\n",
    "            mean=mean_spectra[sample_type]; std=std_spectra[sample_type]\n",
    "            ax.plot(wavelengths_values,mean,color=colors[sample_type],linewidth=2,label=f\"{sample_type} mean\")\n",
    "            ax.fill_between(wavelengths_values,mean-std,mean+std,color=colors[sample_type],alpha=0.2)\n",
    "\n",
    "        \n",
    "        # Continue Option 4 plotting\n",
    "        ax.scatter(wavelengths_values[list(common_maxima)],\n",
    "                   [mean_spectra[t][list(common_maxima)] for t in mean_spectra][0],\n",
    "                   marker='x', s=120, color='black', linewidths=3,\n",
    "                   label=\"Common maxima\")\n",
    "\n",
    "        ax.scatter(wavelengths_values[list(common_minima)],\n",
    "                   [mean_spectra[t][list(common_minima)] for t in mean_spectra][0],\n",
    "                   marker='x', s=120, color='purple', linewidths=3,\n",
    "                   label=\"Common minima\")\n",
    "\n",
    "        ax.set_title(\"Spectra with Common Maxima and Minima\", fontsize=16, fontweight='bold')\n",
    "        ax.set_xlabel(\"Wavelength (nm)\", fontsize=14)\n",
    "        ax.set_ylabel(\"Absorbance (AU)\", fontsize=14)\n",
    "        ax.legend(fontsize=12, loc=\"upper right\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # --- Option 5 ---\n",
    "    elif option == \"Option 5: Summary statistics for common peaks and valleys\":\n",
    "        st.subheader(\"üìë Summary Statistics for Common Peaks and Valleys\")\n",
    "        st.write(\"Below is the summary DataFrame of common peaks and valleys across sample types.\")\n",
    "        st.dataframe(compeak_summary)\n",
    "\n",
    "    # --- Option 6 ---\n",
    "    elif option == \"Option 6: Correlation? You decide\":\n",
    "        st.subheader(\"üîó Correlation Analysis\")\n",
    "        st.write(\"Heatmap showing correlation of absorbance at common wavelengths with TS, TSS, and Turbidity.\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(18,12))  # larger heatmap\n",
    "        sns.heatmap(\n",
    "            physio_corr,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cbar_kws={'label': 'Correlation coefficient'},\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(\"Correlation of Absorbance at Common Wavelengths with TS, TSS, and Turbidity\",\n",
    "                     fontsize=18, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Wavelengths\", fontsize=14)\n",
    "        ax.set_ylabel(\"Physiochemical Parameters\", fontsize=14)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "def page4():\n",
    "    st.title(\"Imputing Missing Values üß©\")\n",
    "\n",
    "    # Placeholder explanatory text\n",
    "    st.markdown(\n",
    "        \"<p style='color:black; font-size:16px;'>\"\n",
    "        \"Placeholder: Describe the imputation technique used (e.g., KNN, mean, median) and why it was chosen.\"\n",
    "        \"</p>\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    # --- Compare Original vs Imputed TSS ---\n",
    "    fig_tss = plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(tss_not_missing.dropna(), kde=True, color='blue', alpha=0.5, label='Original (non-missing)')\n",
    "    sns.histplot(tss_imputed['TSS (% w/v)'], kde=True, color='red', alpha=0.5, label='Imputed')\n",
    "    plt.title('Distribution of Original vs Imputed TSS')\n",
    "    plt.legend()\n",
    "    st.pyplot(fig_tss)\n",
    "\n",
    "    # --- Compare Original vs Imputed Turbidity ---\n",
    "    fig_turb = plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(turb_not_missing.dropna(), kde=True, color='blue', alpha=0.5, label='Original (non-missing)')\n",
    "    sns.histplot(turb_imputed['Turbidity (NTU)'], kde=True, color='red', alpha=0.5, label='Imputed')\n",
    "    plt.title('Distribution of Original vs Imputed Turbidity (NTU)')\n",
    "    plt.legend()\n",
    "    st.pyplot(fig_turb)\n",
    "\n",
    "def page5():\n",
    "    st.title(\"Principal Component Analysis ‚Äì Are my samples discernibly different? üîç\")\n",
    "\n",
    "    # Toggle between PCA biplot and Scree plot\n",
    "    view_option = st.radio(\n",
    "        \"Select plot to display:\",\n",
    "        [\"PCA Biplot\", \"Scree Plot\"],\n",
    "        horizontal=True\n",
    "    )\n",
    "\n",
    "    # --- PCA Biplot ---\n",
    "    if view_option == \"PCA Biplot\":\n",
    "        st.markdown(\n",
    "            \"<p style='color:black; font-size:18px; font-weight:bold;'>\"\n",
    "            \"Placeholder: Explain PCA separation of sample types and how loadings arrows show feature contributions.\"\n",
    "            \"</p>\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Color-coded PCA (red, green, blue)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pca_df[\"PC1\"], y=pca_df[\"PC2\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=12),\n",
    "            text=pca_df[\"Type\"],\n",
    "            name=\"Color-coded PCA\",\n",
    "            marker_color=pca_df[\"Type\"].map({\n",
    "                \"Clean Water\": \"blue\",\n",
    "                \"Dirty Water\": \"red\",\n",
    "                \"Treated Water\": \"green\"\n",
    "            })\n",
    "        ))\n",
    "\n",
    "        # Uncoded PCA (all gray)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pca_df[\"PC1\"], y=pca_df[\"PC2\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=12, color=\"gray\"),\n",
    "            text=pca_df[\"Type\"],\n",
    "            name=\"Uncoded PCA\"\n",
    "        ))\n",
    "\n",
    "        # Variance directional arrows (loadings for PC1 and PC2 only)\n",
    "        loadings = pca.components_.T[:, :2]\n",
    "        for i, feature in enumerate(X.columns):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[0, loadings[i, 0] * 3],\n",
    "                y=[0, loadings[i, 1] * 3],\n",
    "                mode=\"lines+text\",\n",
    "                line=dict(color=\"black\"),\n",
    "                text=[None, feature],\n",
    "                textposition=\"top center\",\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "        # Dropdown menu at top middle\n",
    "        fig.update_layout(\n",
    "            updatemenus=[\n",
    "                dict(\n",
    "                    buttons=list([\n",
    "                        dict(label=\"Color-coded PCA\",\n",
    "                             method=\"update\",\n",
    "                             args=[{\"visible\": [True, False] + [True] * len(X.columns)},\n",
    "                                   {\"title\": \"PCA Biplot (Color-coded by Sample Type)\"}]),\n",
    "                        dict(label=\"Uncoded PCA\",\n",
    "                             method=\"update\",\n",
    "                             args=[{\"visible\": [False, True] + [True] * len(X.columns)},\n",
    "                                   {\"title\": \"PCA Biplot (Uncoded)\"}])\n",
    "                    ]),\n",
    "                    direction=\"down\",\n",
    "                    showactive=True,\n",
    "                    x=0.5, y=1.15,   # üîë top middle\n",
    "                    xanchor=\"center\", yanchor=\"top\"\n",
    "                )\n",
    "            ],\n",
    "            title=\"PCA Biplot\",\n",
    "            xaxis_title=f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\",\n",
    "            yaxis_title=f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\",\n",
    "            height=1000, width=1200,\n",
    "            plot_bgcolor=\"white\", paper_bgcolor=\"white\"\n",
    "        )\n",
    "\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # --- Scree Plot ---\n",
    "\n",
    "    elif view_option == \"Scree Plot\":\n",
    "        st.markdown(\n",
    "            \"<p style='color:black; font-size:18px; font-weight:bold;'>\"\n",
    "            \"Placeholder: Explain how much variance each principal component explains (scree plot).\"\n",
    "            \"</p>\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "    \n",
    "        # Scree plot of explained variance (scatter + line)\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[f\"PC{i+1}\" for i in range(pca.n_components_)],\n",
    "            y=pca.explained_variance_ratio_ * 100,\n",
    "            mode=\"lines+markers\",   # üîë scatter with lines\n",
    "            marker=dict(size=10, color=\"purple\"),\n",
    "            line=dict(color=\"purple\", width=2),\n",
    "            name=\"Variance Explained\"\n",
    "        ))\n",
    "    \n",
    "        fig.update_layout(\n",
    "            title=\"Scree Plot ‚Äì Variance Explained by Principal Components\",\n",
    "            xaxis_title=\"Principal Components\",\n",
    "            yaxis_title=\"Variance Explained (%)\",\n",
    "            height=900, width=1200,\n",
    "            plot_bgcolor=\"white\", paper_bgcolor=\"white\"\n",
    "        )\n",
    "    \n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "def page6():\n",
    "    st.title(\"Linear Regression Using All Common Wavelengths üìà\")\n",
    "\n",
    "    # --- Define predictors for full spectrum ---\n",
    "    X_full_lr = waves_physio.drop(columns=[\n",
    "        \"Sample_ID\",\"Type\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"\n",
    "    ])\n",
    "    y_tss_lr = waves_physio[\"TSS (% w/v)\"]\n",
    "    y_turb_lr = waves_physio[\"Turbidity (NTU)\"]\n",
    "\n",
    "    # --- Standardize predictors ---\n",
    "    scaler_lr = StandardScaler()\n",
    "    X_scaled_lr = scaler_lr.fit_transform(X_full_lr)\n",
    "\n",
    "    # --- Fit regression models ---\n",
    "    model_tss_lr = LinearRegression().fit(X_scaled_lr, y_tss_lr)\n",
    "    model_turb_lr = LinearRegression().fit(X_scaled_lr, y_turb_lr)\n",
    "\n",
    "    # --- Predictions ---\n",
    "    y_tss_pred_lr = model_tss_lr.predict(X_scaled_lr)\n",
    "    y_turb_pred_lr = model_turb_lr.predict(X_scaled_lr)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    tss_r2_lr = r2_score(y_tss_lr, y_tss_pred_lr)\n",
    "    tss_rmse_lr = np.sqrt(mean_squared_error(y_tss_lr, y_tss_pred_lr))\n",
    "    turb_r2_lr = r2_score(y_turb_lr, y_turb_pred_lr)\n",
    "    turb_rmse_lr = np.sqrt(mean_squared_error(y_turb_lr, y_turb_pred_lr))\n",
    "\n",
    "    st.subheader(\"Model Performance Metrics\")\n",
    "    st.write(f\"TSS R¬≤:   {tss_r2_lr:.3f}\")\n",
    "    st.write(f\"TSS RMSE: {tss_rmse_lr:.3f}\")\n",
    "    st.write(f\"Turbidity R¬≤:   {turb_r2_lr:.3f}\")\n",
    "    st.write(f\"Turbidity RMSE: {turb_rmse_lr:.3f}\")\n",
    "\n",
    "    # --- Predicted vs Actual Plots ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "    axes[0].scatter(y_tss_lr, y_tss_pred_lr, color=\"steelblue\", alpha=0.7)\n",
    "    axes[0].plot([y_tss_lr.min(), y_tss_lr.max()], [y_tss_lr.min(), y_tss_lr.max()], 'k--', lw=1)\n",
    "    axes[0].set_title(\"TSS: Predicted vs Actual\")\n",
    "    axes[0].set_xlabel(\"Actual TSS (% w/v)\")\n",
    "    axes[0].set_ylabel(\"Predicted TSS\")\n",
    "\n",
    "    axes[1].scatter(y_turb_lr, y_turb_pred_lr, color=\"darkorange\", alpha=0.7)\n",
    "    axes[1].plot([y_turb_lr.min(), y_turb_lr.max()], [y_turb_lr.min(), y_turb_lr.max()], 'k--', lw=1)\n",
    "    axes[1].set_title(\"Turbidity: Predicted vs Actual\")\n",
    "    axes[1].set_xlabel(\"Actual Turbidity (NTU)\")\n",
    "    axes[1].set_ylabel(\"Predicted Turbidity\")\n",
    "\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    # --- Regression Equations ---\n",
    "    eq_tss_lr = print_equation(model_tss_lr, X_full_lr, \"TSS (% w/v)\")\n",
    "    eq_turb_lr = print_equation(model_turb_lr, X_full_lr, \"Turbidity (NTU)\")\n",
    "    st.subheader(\"Regression Equations\")\n",
    "    st.markdown(f\"**TSS Equation:** {eq_tss_lr}\")\n",
    "    st.markdown(f\"**Turbidity Equation:** {eq_turb_lr}\")\n",
    "\n",
    "    # --- Coefficient Plots ---\n",
    "    coef_df_lr = pd.DataFrame({\n",
    "        \"Wavelength\": X_full_lr.columns,\n",
    "        \"TSS_Coefficient\": model_tss_lr.coef_,\n",
    "        \"Turbidity_Coefficient\": model_turb_lr.coef_\n",
    "    })\n",
    "\n",
    "    fig1, ax1 = plt.subplots(figsize=(10,6))\n",
    "    ax1.bar(coef_df_lr[\"Wavelength\"], coef_df_lr[\"TSS_Coefficient\"], color=\"steelblue\")\n",
    "    ax1.set_title(\"TSS Regression Coefficients\")\n",
    "    ax1.set_ylabel(\"Coefficient Value\")\n",
    "    ax1.set_xticklabels(coef_df_lr[\"Wavelength\"], rotation=90)\n",
    "    st.pyplot(fig1)\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=(10,6))\n",
    "    ax2.bar(coef_df_lr[\"Wavelength\"], coef_df_lr[\"Turbidity_Coefficient\"], color=\"darkorange\")\n",
    "    ax2.set_title(\"Turbidity Regression Coefficients\")\n",
    "    ax2.set_ylabel(\"Coefficient Value\")\n",
    "    ax2.set_xticklabels(coef_df_lr[\"Wavelength\"], rotation=90)\n",
    "    st.pyplot(fig2)\n",
    "\n",
    "def page7():\n",
    "    st.title(\"Multivariate Regression Comparison üìä\")\n",
    "\n",
    "    # --- Introductory blurb ---\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        ### Why Multivariate Regression?\n",
    "        In this analysis we focused on absorbance values at five wavelengths: **951.46 nm, 957.655 nm, 970.044 nm,\n",
    "        976.238 nm, and 1186.846 nm**. These were selected because single‚Äëvariable decomposition showed they consistently\n",
    "        explained the most variance in both TSS and Turbidity, particularly the cluster in the 950‚Äì976 nm region.\n",
    "\n",
    "        While individual wavelengths can provide useful signals, water quality is influenced by overlapping spectral\n",
    "        features. **Multivariate regression allows us to combine multiple predictors simultaneously**, capturing\n",
    "        complementary information and reducing redundancy. By comparing performance against the full spectrum model,\n",
    "        we can evaluate whether a reduced set of wavelengths is sufficient for accurate prediction, balancing\n",
    "        interpretability with predictive power.\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    # --- Define predictors ---\n",
    "    top_wavelengths = [\n",
    "        \"Wavelength_951.46\",\"Wavelength_957.655\",\n",
    "        \"Wavelength_970.044\",\"Wavelength_976.238\",\n",
    "        \"Wavelength_1186.846\"\n",
    "    ]\n",
    "    X_top_comp = waves_physio[top_wavelengths]\n",
    "    X_full_comp = waves_physio.drop(columns=[\n",
    "        \"Sample_ID\",\"Type\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"\n",
    "    ])\n",
    "    y_tss_comp = waves_physio[\"TSS (% w/v)\"]\n",
    "    y_turb_comp = waves_physio[\"Turbidity (NTU)\"]\n",
    "\n",
    "    # --- Standardize predictors ---\n",
    "    scaler_comp = StandardScaler()\n",
    "    X_top_scaled = scaler_comp.fit_transform(X_top_comp)\n",
    "    X_full_scaled = scaler_comp.fit_transform(X_full_comp)\n",
    "\n",
    "    # --- Fit models ---\n",
    "    model_tss_top = LinearRegression().fit(X_top_scaled, y_tss_comp)\n",
    "    model_turb_top = LinearRegression().fit(X_top_scaled, y_turb_comp)\n",
    "    model_tss_full = LinearRegression().fit(X_full_scaled, y_tss_comp)\n",
    "    model_turb_full = LinearRegression().fit(X_full_scaled, y_turb_comp)\n",
    "\n",
    "    # --- Evaluate performance ---\n",
    "    def evaluate(model, X, y):\n",
    "        y_pred = model.predict(X)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "        return r2, rmse\n",
    "\n",
    "    tss_r2_top, tss_rmse_top = evaluate(model_tss_top, X_top_scaled, y_tss_comp)\n",
    "    tss_r2_full, tss_rmse_full = evaluate(model_tss_full, X_full_scaled, y_tss_comp)\n",
    "    turb_r2_top, turb_rmse_top = evaluate(model_turb_top, X_top_scaled, y_turb_comp)\n",
    "    turb_r2_full, turb_rmse_full = evaluate(model_turb_full, X_full_scaled, y_turb_comp)\n",
    "   # --- Display metrics ---\n",
    "    st.subheader(\"Single Value Decomposition Results\")\n",
    "    st.plotly_chart(fig_tss, use_container_width=True)\n",
    "    st.plotly_chart(fig_turb, use_container_width=True)\n",
    "    \n",
    "    # --- Display metrics ---\n",
    "    st.subheader(\"Model Performance\")\n",
    "    st.markdown(f\"**TSS (Top wavelengths):** R¬≤ = {tss_r2_top:.3f}, RMSE = {tss_rmse_top:.3f}\")\n",
    "    st.markdown(f\"**TSS (Full spectrum):** R¬≤ = {tss_r2_full:.3f}, RMSE = {tss_rmse_full:.3f}\")\n",
    "    st.markdown(f\"**Turbidity (Top wavelengths):** R¬≤ = {turb_r2_top:.3f}, RMSE = {turb_rmse_top:.3f}\")\n",
    "    st.markdown(f\"**Turbidity (Full spectrum):** R¬≤ = {turb_r2_full:.3f}, RMSE = {turb_rmse_full:.3f}\")\n",
    "\n",
    "    # --- Observations blurb with emojis ---\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        ### ‚úÖ Observations\n",
    "\n",
    "        üåä **Strongest performers for Turbidity (highest R¬≤):**\n",
    "        - 951.46 nm ‚Üí Turbidity R¬≤ = 0.492  \n",
    "        - 957.655 nm ‚Üí Turbidity R¬≤ = 0.491  \n",
    "        - 970.044 nm ‚Üí Turbidity R¬≤ = 0.488  \n",
    "        - These three are clustered in the **950‚Äì970 nm region**, suggesting a spectral band that is particularly informative for turbidity.\n",
    "\n",
    "        üìä **Strongest performers for TSS (highest R¬≤):**\n",
    "        - 957.655 nm ‚Üí TSS R¬≤ = 0.385  \n",
    "        - 951.46 nm ‚Üí TSS R¬≤ = 0.384  \n",
    "        - 970.044 nm ‚Üí TSS R¬≤ = 0.383  \n",
    "        - Again, the same **950‚Äì970 nm region** dominates.\n",
    "\n",
    "        ‚ö†Ô∏è **Weaker predictors:**\n",
    "        - 1267.373 nm and 1453.203 nm have noticeably lower R¬≤ values (especially for turbidity, 0.440 and 0.341).  \n",
    "        - These wavelengths add less explanatory power individually.\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "    # --- Visualize coefficients for Top wavelengths ---\n",
    "    st.subheader(\"Feature Importance (Top Wavelengths)\")\n",
    "    coef_tss = dict(zip(top_wavelengths, model_tss_top.coef_))\n",
    "    coef_turb = dict(zip(top_wavelengths, model_turb_top.coef_))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.barplot(x=list(coef_tss.keys()), y=list(coef_tss.values()), ax=axes[0])\n",
    "    axes[0].set_title(\"TSS Coefficients\")\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    sns.barplot(x=list(coef_turb.keys()), y=list(coef_turb.values()), ax=axes[1])\n",
    "    axes[1].set_title(\"Turbidity Coefficients\")\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    st.pyplot(fig)\n",
    "\n",
    "# Page 8: Prediction Tool\n",
    "def page8():\n",
    "    st.title(\"Predict TSS and Turbidity from Absorbance üåä\")\n",
    "\n",
    "    # Placeholder explanatory text\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <p style='color:black; font-size:18px; font-weight:bold;'>\n",
    "            Because we found that the <span style='color:darkorange; font-weight:bold;'>full spectrum model</span> \n",
    "            yielded a higher R¬≤ value, we will use all the common wavelength absorbances to predict TSS and Turbidity.\n",
    "        </p>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Option 1: Manual Entry ---\n",
    "    st.subheader(\"Manual Entry of Absorbance Values\")\n",
    "    user_inputs = {}\n",
    "    for feature in X.columns:\n",
    "        user_inputs[feature] = st.number_input(\n",
    "            f\"Absorbance at {feature}\",\n",
    "            value=0.0,\n",
    "            format=\"%.3f\"\n",
    "        )\n",
    "\n",
    "    if st.button(\"Predict from Manual Entry\"):\n",
    "        input_df = pd.DataFrame([user_inputs])\n",
    "        input_scaled = scaler.transform(input_df)\n",
    "        tss_pred = model_tss.predict(input_scaled)[0]\n",
    "        turb_pred = model_turb.predict(input_scaled)[0]\n",
    "\n",
    "        st.write(f\"**Predicted TSS (% w/v):** {tss_pred:.3f}\")\n",
    "        st.write(f\"**Predicted Turbidity (NTU):** {turb_pred:.3f}\")\n",
    "\n",
    "    # --- Option 2: Upload Excel File ---\n",
    "    st.subheader(\"Upload Excel File with Absorbance Values\")\n",
    "    uploaded_file = st.file_uploader(\"Upload Excel file\", type=[\"xlsx\", \"xls\"])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        try:\n",
    "            df_uploaded = pd.read_excel(uploaded_file)\n",
    "            st.write(\"Preview of uploaded data:\")\n",
    "            st.dataframe(df_uploaded.head())\n",
    "\n",
    "            # Ensure only predictor columns are used\n",
    "            df_predictors = df_uploaded[X.columns]\n",
    "\n",
    "            # Scale inputs\n",
    "            input_scaled = scaler.transform(df_predictors)\n",
    "\n",
    "            # Predict TSS and Turbidity\n",
    "            tss_preds = model_tss.predict(input_scaled)\n",
    "            turb_preds = model_turb.predict(input_scaled)\n",
    "\n",
    "            # Combine results\n",
    "            results_df = df_uploaded.copy()\n",
    "            results_df[\"Predicted TSS (% w/v)\"] = tss_preds\n",
    "            results_df[\"Predicted Turbidity (NTU)\"] = turb_preds\n",
    "\n",
    "            st.subheader(\"Predicted Results\")\n",
    "            st.dataframe(results_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error processing file: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# Main App Navigation\n",
    "# -------------------------\n",
    "st.sidebar.title(\"Navigation\")\n",
    "\n",
    "section = st.sidebar.radio(\"Choose section:\", [\"üìö Project Overview\", \"‚öôÔ∏è App Tool\"])\n",
    "\n",
    "if section == \"üìö Project Overview\":\n",
    "    page = st.sidebar.selectbox(\n",
    "        \"Select a page:\",\n",
    "        [\n",
    "            \"Project Overview\",\n",
    "            \"IDA: Initial Data Analysis\",\n",
    "            \"EDA: Exploratory Data Analysis\",\n",
    "            \"Imputing Missing Values\",\n",
    "            \"PCA ‚Äì Are my samples discernibly different?\",\n",
    "            \"Linear Regression Using All Common Wavelengths\",\n",
    "            \"Multivariate Regression Comparison\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if page == \"Project Overview\":\n",
    "        page1()\n",
    "    elif page == \"IDA: Initial Data Analysis\":\n",
    "        page2()\n",
    "    elif page == \"EDA: Exploratory Data Analysis\":\n",
    "        page3()\n",
    "    elif page == \"Imputing Missing Values\":\n",
    "        page4()\n",
    "    elif page == \"PCA ‚Äì Are my samples discernibly different?\":\n",
    "        page5()\n",
    "    elif page == \"Linear Regression Using All Common Wavelengths\":\n",
    "        page6()\n",
    "    elif page == \"Multivariate Regression Comparison\":\n",
    "        page7()\n",
    "\n",
    "\n",
    "elif section == \"‚öôÔ∏è App Tool\":\n",
    "    page8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "026d43cd-4331-415f-99ff-79290ac171c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streamlit==1.45.1\n",
      "pandas==2.2.3\n",
      "numpy==2.1.3\n",
      "matplotlib==3.10.0\n",
      "seaborn==0.13.2\n",
      "scikit-learn==1.6.1\n",
      "openpyxl==3.1.5\n",
      "scipy==1.15.3\n",
      "plotly==5.24.1\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "packages = [\"streamlit\",\"pandas\",\"numpy\",\"matplotlib\",\"seaborn\",\n",
    "            \"scikit-learn\",\"openpyxl\",\"scipy\",\"plotly\"]\n",
    "\n",
    "for pkg in packages:\n",
    "    print(f\"{pkg}=={pkg_resources.get_distribution(pkg).version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74a921-39cd-483f-9735-3a468d61ecad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
