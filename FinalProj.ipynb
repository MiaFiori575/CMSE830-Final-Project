{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b840b0b-45fe-495c-abbe-81268110a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2beedff7-7bf7-4529-b9b6-8dd5c80f638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FinalProj.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FinalProj.py\n",
    "#loading data into the data frame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import openpyxl\n",
    "from scipy.signal import find_peaks\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import streamlit as st\n",
    "\n",
    "# Loading data sheets according to sample type\n",
    "treated_waves= pd.read_excel(\"Sample no Filter.xlsx\", sheet_name=\"Samples\", header=None)\n",
    "clean_waves= pd.read_excel(\"Sample no Filter.xlsx\", sheet_name=\"Clean Water\", header=None)\n",
    "bw_waves= pd.read_excel(\"Sample no Filter.xlsx\", sheet_name=\"Blackwater\", header=None)\n",
    "\n",
    "#grouping triplicate measurements and taking the average, then replacing the triplicate measurements with the average in the\n",
    "#data frame\n",
    "\n",
    "##blackwater\n",
    "# Assume first column = sample names, rest = numeric data\n",
    "bw_waves.columns = [\"Sample\"] + [f\"Col{i}\" for i in range(1, bw_waves.shape[1])]\n",
    "# Extract group prefix (BW1, BW2, BW3...)\n",
    "bw_waves[\"Group\"] = bw_waves[\"Sample\"].str.extract(r'^(BW\\d+)')\n",
    "# Compute averages across all numeric columns per group\n",
    "bw_waves = bw_waves.groupby(\"Group\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "\n",
    "##treated_water\n",
    "treated_waves = treated_waves.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Rename columns: first col = Sample, rest = numeric data\n",
    "treated_waves.columns = [\"Sample\"] + [f\"Col{i}\" for i in range(1, treated_waves.shape[1])]\n",
    "\n",
    "# Extract group prefix: everything before the first dash\n",
    "treated_waves[\"Group\"] = treated_waves[\"Sample\"].astype(str).str.extract(r'^([A-Za-z0-9]+)')\n",
    "\n",
    "# Compute averages across all numeric columns per group\n",
    "treated_waves = treated_waves.groupby(\"Group\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "##clean_water\n",
    "clean_waves = clean_waves.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# Rename columns: first col = Sample, rest = numeric data\n",
    "clean_waves.columns = [\"Sample\"] + [f\"Col{i}\" for i in range(1, clean_waves.shape[1])]\n",
    "\n",
    "# Extract group prefix: everything before the first dash\n",
    "clean_waves[\"Group\"] = clean_waves[\"Sample\"].astype(str).str.extract(r'^([A-Za-z0-9]+)')\n",
    "\n",
    "# Compute averages across all numeric columns per group\n",
    "clean_waves = clean_waves.groupby(\"Group\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "#merging related data\n",
    "tw_bw_clean_waves = pd.concat([treated_waves, bw_waves, clean_waves])\n",
    "\n",
    "\n",
    "#importing turbidity measurements \n",
    "turb = pd.read_excel('physiochemical data.xlsx', sheet_name='Master Physiochem', usecols=['Turbidity (NTU)'])\n",
    "#checking that data was correctly imported\n",
    "\n",
    "#importing total suspended solids measurements \n",
    "tss = pd.read_excel('physiochemical data.xlsx', sheet_name='TSS', usecols=['TSS (% w/v)'])\n",
    "\n",
    "#importing total solids measurements \n",
    "ts = pd.read_excel('physiochemical data.xlsx', sheet_name='TS', usecols=['%TS (w/w)'])\n",
    "\n",
    "#missing and not missing\n",
    "tss_missing = tss[tss.isnull().any(axis=1)]\n",
    "tss_not_missing = tss.dropna()\n",
    "\n",
    "#preparing scalar for KNN\n",
    "scaler = StandardScaler()\n",
    "tss_scaled = pd.DataFrame(scaler.fit_transform(tss_not_missing), columns = tss_not_missing.columns)\n",
    "\n",
    "#intialize and fit KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights ='distance')\n",
    "#here we have to make the scatter plot of the data without missing values so they aren't skewed by missing values, then we overlay the missing values \n",
    "#on the scatter plot \n",
    "#what does .fit do? Training on non-missing data ---it's machine learning and has never seen this data set before, so it has to train\n",
    "imputer.fit(tss_scaled)\n",
    "\n",
    "# function to impute and inverse transform the data\n",
    "def impute_and_inverse_transform(data):\n",
    "    # Ensure 'data' is always a DataFrame with proper column names\n",
    "    scaled_data = pd.DataFrame(scaler.transform(data), columns=data.columns, index=data.index)\n",
    "    imputed_scaled = imputer.transform(scaled_data)\n",
    "    return pd.DataFrame(scaler.inverse_transform(imputed_scaled), columns=data.columns, index=data.index)\n",
    "\n",
    "# impute missing values\n",
    "tss_imputed = impute_and_inverse_transform(tss)\n",
    "\n",
    "#missing and not missing\n",
    "turb_missing = turb[turb.isnull().any(axis=1)]\n",
    "turb_not_missing = turb.dropna()\n",
    "\n",
    "#preparing scalar for KNN\n",
    "scaler = StandardScaler()\n",
    "turb_scaled = pd.DataFrame(scaler.fit_transform(turb_not_missing), columns = turb_not_missing.columns)\n",
    "\n",
    "#intialize and fit KNN imputer\n",
    "#imputer = KNNImputer(n_neighbors=5)\n",
    "#modded above function to include 10 neighbors and weight the imputed value depending on how close the points were-the closer the point the more it \n",
    "#affects the imputing value\n",
    "imputer = KNNImputer(n_neighbors=3, weights ='distance')\n",
    "#here we have to make the scatter plot of the data without missing values so they aren't skewed by missing values, then we overlay the missing values \n",
    "#on the scatter plot \n",
    "#what does .fit do? Training on non-missing data ---it's machine learning and has never seen this data set before, so it has to train\n",
    "imputer.fit(turb_scaled)\n",
    "# impute missing values\n",
    "turb_imputed = impute_and_inverse_transform(turb)\n",
    "\n",
    "#tss and turb data was imputed, no data missing for ts\n",
    "\n",
    "tw_tss = tss_imputed[0:96]\n",
    "tw_ts = ts[0:96]\n",
    "tw_turb = turb_imputed[0:96]\n",
    "\n",
    "bw_tss = tss_imputed[96:106]\n",
    "bw_ts = ts[96:106]\n",
    "bw_turb = turb_imputed[96:106]\n",
    "\n",
    "#reading sample names\n",
    "sample_IDs = pd.read_excel('physiochemical data.xlsx', sheet_name='Master Physiochem', usecols=['Sample'])\n",
    "\n",
    "#merging related data\n",
    "tw_physio = pd.concat([tw_tss, tw_ts,tw_turb], axis=1)\n",
    "bw_physio = pd.concat([bw_tss, bw_ts,bw_turb], axis=1)\n",
    "tw_bw_physio = pd.concat([ tw_physio,bw_physio])\n",
    "\n",
    "# Add the Sample IDs as a new column\n",
    "tw_bw_physio.insert(0, 'Sample', sample_IDs['Sample'].values)\n",
    "\n",
    "# Define column names (excluding 'Sample')\n",
    "value_cols = [col for col in tw_bw_physio.columns if col != 'Sample']\n",
    "\n",
    "# Generate random values with different ranges\n",
    "col1 = np.random.uniform(0, 0.00002, size=(7,))   # TSS (% w/v)\n",
    "col2 = np.random.uniform(0, 0.00002, size=(7,))   # %TS (w/w)\n",
    "col3 = np.random.uniform(0, 0.02, size=(7,))     # Turbidity (NTU)\n",
    "\n",
    "# Stack into DataFrame\n",
    "new_df = pd.DataFrame(\n",
    "    np.column_stack([col1, col2, col3]),\n",
    "    columns=value_cols\n",
    ")\n",
    "\n",
    "# Add the Sample IDs as the first column\n",
    "new_df = pd.concat(\n",
    "    [clean_waves[['Group']].rename(columns={'Group':'Sample'}).head(7), new_df],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Append to the original DataFrame\n",
    "tw_bw_clean_physio = pd.concat([tw_bw_physio, new_df], ignore_index=True)\n",
    "\n",
    "# Assume your DataFrame is called df with columns:\n",
    "# ['Sample', 'TSS (% w/v)', '%TS (w/w)', 'Turbidity (NTU)']\n",
    "\n",
    "# Segregating samples according to sample name [treated water (A1, B2, etc), clean (Blank1, Blank2), blackwater feed (BW1, BW2) etc)\n",
    "def classify_sample(sample):\n",
    "    if sample.startswith(\"Blank\"):\n",
    "        return \"Clean Water\"\n",
    "    elif sample.startswith(\"BW\"):\n",
    "        return \"Contaminated Water\"\n",
    "    else:\n",
    "        return \"Treated Water\"\n",
    "\n",
    "# Use classify sample\n",
    "tw_bw_clean_physio['Type'] = tw_bw_clean_physio['Sample'].apply(classify_sample)\n",
    "\n",
    "# Describe statistics in a separate value\n",
    "stats = tw_bw_clean_physio.groupby('Type').describe()\n",
    "\n",
    "\n",
    "grouped_stats = tw_bw_clean_physio.groupby('Type')[['TSS (% w/v)', '%TS (w/w)', 'Turbidity (NTU)']].agg(['mean','std'])\n",
    "\n",
    "# Extract mean and std separately for plotting\n",
    "means = grouped_stats.xs('mean', axis=1, level=1)\n",
    "stds  = grouped_stats.xs('std', axis=1, level=1)\n",
    "\n",
    "#tw_bw_clean_waves['Type'] = tw_bw_clean_waves['Sample'].apply(classify_sample)\n",
    "wavelengths = pd.read_excel(\"Sample no Filter.xlsx\", sheet_name=\"Samples\", nrows=1, header = None)\n",
    "wavelengths = wavelengths.drop(columns=[0])\n",
    "\n",
    "tw_bw_clean_waves['Type'] = tw_bw_clean_waves['Group'].apply(classify_sample)\n",
    "\n",
    "# --- Step 2: Extract wavelength values ---\n",
    "wavelengths_values = wavelengths.values.flatten()  # drop first col if needed\n",
    "\n",
    "# --- Step 1: Compute mean spectra and find maxima/minima indices ---\n",
    "mean_spectra = {}\n",
    "for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "    data = group.drop(columns=['Group','Type']).values\n",
    "    mean_spectra[sample_type] = data.mean(axis=0)\n",
    "\n",
    "# Find maxima/minima for each type\n",
    "maxima_sets = []\n",
    "minima_sets = []\n",
    "for mean in mean_spectra.values():\n",
    "    maxima_idx, _ = find_peaks(mean)\n",
    "    minima_idx, _ = find_peaks(-mean)\n",
    "    maxima_sets.append(maxima_idx)\n",
    "    minima_sets.append(minima_idx)\n",
    "\n",
    "# Function to find common peaks within tolerance\n",
    "def find_common_peaks(sets, tolerance=5):\n",
    "    common = []\n",
    "    ref = sets[0]\n",
    "    for idx in ref:\n",
    "        if all(any(abs(idx - other) <= tolerance for other in s) for s in sets[1:]):\n",
    "            common.append(idx)\n",
    "    return np.array(common)\n",
    "common_maxima = find_common_peaks(maxima_sets, tolerance=5)\n",
    "common_minima = find_common_peaks(minima_sets, tolerance=5)\n",
    "\n",
    "# --- Step 2: Extract wavelengths at these indices ---\n",
    "selected_indices = np.sort(np.concatenate([common_maxima, common_minima]))\n",
    "selected_wavelengths = wavelengths.iloc[0, selected_indices].values\n",
    "\n",
    "# --- Step 3: Build new DataFrame ---\n",
    "rows = []\n",
    "# First row: wavelengths\n",
    "header_row = [\"Sample_ID\"] + selected_wavelengths.tolist()\n",
    "rows.append(header_row)\n",
    "\n",
    "# Subsequent rows: sample ID + values at selected indices\n",
    "for _, row in tw_bw_clean_waves.iterrows():\n",
    "    sample_id = row['Group']\n",
    "    values = row.drop(labels=['Group','Type']).values[selected_indices]\n",
    "    rows.append([sample_id] + values.tolist())\n",
    "\n",
    "# Convert to DataFrame\n",
    "compeak_sum = pd.DataFrame(rows)\n",
    "\n",
    "# --- Step 4: Optional formatting ---\n",
    "compeak_sum.columns = [\"Sample_ID\"] + [f\"Wavelength_{w}\" for w in selected_wavelengths]\n",
    "# Compute summary statistics for each sample type\n",
    "summary_stats = []\n",
    "\n",
    "for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "    data = group.drop(columns=['Group','Type']).values[:, selected_indices]\n",
    "    \n",
    "    stats = {\n",
    "        \"Sample_Type\": sample_type,\n",
    "        \"N_samples\": data.shape[0]\n",
    "    }\n",
    "    \n",
    "    # For each wavelength, compute stats\n",
    "    for i, wl in enumerate(selected_wavelengths):\n",
    "        stats[f\"{wl}_mean\"]   = data[:, i].mean()\n",
    "        stats[f\"{wl}_std\"]    = data[:, i].std()\n",
    "        stats[f\"{wl}_min\"]    = data[:, i].min()\n",
    "        stats[f\"{wl}_max\"]    = data[:, i].max()\n",
    "        stats[f\"{wl}_median\"] = np.median(data[:, i])\n",
    "    \n",
    "    summary_stats.append(stats)\n",
    "\n",
    "# --- Step 4: Convert to DataFrame ---\n",
    "compeak_summary = pd.DataFrame(summary_stats)\n",
    "# Ensure both DataFrames have Sample_ID as the first column\n",
    "if compeak_sum.columns[0] != \"Sample_ID\":\n",
    "    compeak_sum = compeak_sum.rename(columns={compeak_sum.columns[0]: \"Sample_ID\"})\n",
    "if tw_bw_clean_physio.columns[0] != \"Sample_ID\":\n",
    "    tw_bw_clean_physio = tw_bw_clean_physio.rename(columns={tw_bw_clean_physio.columns[0]: \"Sample_ID\"})\n",
    "\n",
    "# --- Step 1: Merge on Sample_ID ---\n",
    "merged_df = pd.merge(\n",
    "    tw_bw_clean_physio,\n",
    "    compeak_sum,\n",
    "    on=\"Sample_ID\",\n",
    "    how=\"inner\"   # only keep matching IDs\n",
    ")\n",
    "\n",
    "# --- Step 2: Reorder columns so tw_bw_clean_physio comes first ---\n",
    "ordered_cols = [\"Sample_ID\"] + \\\n",
    "               [col for col in tw_bw_clean_physio.columns if col != \"Sample_ID\"] + \\\n",
    "               [col for col in compeak_sum.columns if col != \"Sample_ID\"]\n",
    "\n",
    "waves_physio = merged_df[ordered_cols]\n",
    "\n",
    "# --- Step 1: Ensure Sample_ID alignment ---\n",
    "if compeak_sum.columns[0] != \"Sample_ID\":\n",
    "    compeak_sum = compeak_sum.rename(columns={compeak_sum.columns[0]: \"Sample_ID\"})\n",
    "if tw_bw_clean_physio.columns[0] != \"Sample_ID\":\n",
    "    tw_bw_clean_physio = tw_bw_clean_physio.rename(columns={tw_bw_clean_physio.columns[0]: \"Sample_ID\"})\n",
    "\n",
    "# --- Step 2: Merge absorbance + physiochemical data ---\n",
    "# TSS (% w/v)     %TS (w/w)  Turbidity (NTU) \n",
    "merged_df = pd.merge(\n",
    "    tw_bw_clean_physio[[\"Sample_ID\",\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"]],\n",
    "    compeak_sum,\n",
    "    on=\"Sample_ID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# --- Step 3: Compute correlation matrix ---\n",
    "corr_matrix = merged_df.drop(columns=[\"Sample_ID\"]).corr()\n",
    "\n",
    "# --- Step 4: Extract correlations of TS, TSS, Turbidity vs wavelengths ---\n",
    "physio_corr = corr_matrix.loc[[\"%TS (w/w)\",\"TSS (% w/v)\",\"Turbidity (NTU)\"], compeak_sum.columns[1:]]\n",
    "# --- Step 1: Keep only numeric columns ---\n",
    "# Drop Sample_ID and any non-numeric columns like 'Type'\n",
    "#X = waves_physio.select_dtypes(include=[np.number])\n",
    "\n",
    "# --- Step 2: Standardize data ---\n",
    "#X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# --- Step 3: Run PCA ---\n",
    "#pca = PCA()\n",
    "#X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# --- Step 3: Create DataFrame with PCA results + sample type ---\n",
    "#pca_df = pd.DataFrame({\n",
    "#    \"PC1\": X_pca[:,0],\n",
    "#    \"PC2\": X_pca[:,1],\n",
    "#    \"Type\": waves_physio[\"Type\"].values\n",
    "#})\n",
    "\n",
    "# Drop Sample_ID and any non-numeric columns like 'Type'\n",
    "X = waves_physio.select_dtypes(include=[np.number])\n",
    "\n",
    "# --- Step 2: Standardize data ---\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# --- Step 3: Run PCA ---\n",
    "pca = PCA(n_components=8)   # explicitly set number of components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# --- Step 4: Create DataFrame with PCA results + sample type ---\n",
    "# Keep ALL components for scree plot, but we‚Äôll only use PC1/PC2 for biplot\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(pca.n_components_)])\n",
    "pca_df[\"Type\"] = waves_physio[\"Type\"].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Page 1: Project Overview\n",
    "# -------------------------\n",
    "def page1():\n",
    "    st.title(\"üåäüíßüß™ Absorbance ‚Üí TSS & Turbidity Project üö∞‚öóÔ∏è\")\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    ### üåç Why This Project?\n",
    "    Water quality is a critical issue worldwide üåä. Suspended solids and turbidity directly affect ecosystems üêü,\n",
    "    drinking water safety üö∞, and wastewater treatment efficiency üè≠. By using absorbance data at specific wavelengths,\n",
    "    we can build predictive models that help monitor and manage water quality more effectively üíß.\n",
    "\n",
    "    ### üî¨ Scientific Motivation\n",
    "    Spectroscopy provides a rapid, non-destructive way to analyze water samples üß™. Instead of relying solely on\n",
    "    traditional lab methods ‚öóÔ∏è, absorbance readings can be transformed into meaningful predictions of TSS and Turbidity.\n",
    "    This bridges environmental science üå± with data science üìä, creating scalable solutions for water monitoring.\n",
    "\n",
    "    ### üöÄ Personal Goal\n",
    "    My aim is to demonstrate how reproducible modeling workflows can connect raw spectral data üìà with real-world\n",
    "    water quality outcomes üåä. This project is both a scientific exploration üî¨ and a practical tool for\n",
    "    wastewater management üè≠ and clean water initiatives üíß.\n",
    "    \"\"\")\n",
    "\n",
    "# -------------------------\n",
    "# Page 2: IDA\n",
    "# -------------------------\n",
    "def page2():\n",
    "    st.title(\"IDA\")\n",
    "\n",
    "    option = st.selectbox(\n",
    "        \"Select what to view:\",\n",
    "        [\n",
    "            \"Option 1: Data collection and importation\",\n",
    "            \"Option 2: Data cleaning and preprocessing\",\n",
    "            \"Option 3: Basic descriptive statistics\",\n",
    "            \"Option 4: Missing data analysis\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if option == \"Option 1: Data collection and importation\":\n",
    "        st.subheader(\"üì• Data Collection and Importation\")\n",
    "        st.write(\"Placeholder text: Describe how absorbance data was collected.\")\n",
    "        st.dataframe(tw_bw_clean_waves.head())\n",
    "\n",
    "    elif option == \"Option 2: Data cleaning and preprocessing\":\n",
    "        st.subheader(\"üßπ Data Cleaning and Preprocessing\")\n",
    "        st.write(\"Placeholder text: Describe splitting between blackwater, treated water, and clean water.\")\n",
    "        st.dataframe(tw_bw_clean_physio.head())\n",
    "\n",
    "    elif option == \"Option 3: Basic descriptive statistics\":\n",
    "        st.subheader(\"üìä Basic Descriptive Statistics\")\n",
    "        st.dataframe(stats)\n",
    "\n",
    "    elif option == \"Option 4: Missing data analysis\":\n",
    "        st.subheader(\"‚ùì Missing Data Analysis\")\n",
    "        merged_turbtssts = pd.concat([turb, tss, ts], axis=1)\n",
    "        fig, ax = plt.subplots(figsize=(12,18))  # larger heatmap\n",
    "        sns.heatmap(merged_turbtssts.isna(), cmap=\"rocket\", ax=ax)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "# -------------------------\n",
    "# Page 3: EDA\n",
    "# -------------------------\n",
    "def page3():\n",
    "    st.title(\"EDA: Exploratory Data Analysis\")\n",
    "    st.markdown(\n",
    "    \"<h3 style='color:#C71585; font-weight:bold;'>Click the expand button in the top right corner of the figure for a zoomed in view! - üîç</h3>\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "\n",
    "    option = st.selectbox(\n",
    "        \"Select what to view:\",\n",
    "        [\n",
    "            \"Option 1: Visualizing means of physiochemical characteristics vs sample type\",\n",
    "            \"Option 2: Visualizing spectral fingerprints according to sample types\",\n",
    "            \"Option 3: Separate sample types and their absorbance plots-Interactive!\",\n",
    "            \"Option 4: Common peaks and valleys\",\n",
    "            \"Option 5: Summary statistics for common peaks and valleys\",\n",
    "            \"Option 6: Correlation? You decide\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if option == \"Option 1: Visualizing means of physiochemical characteristics vs sample type\":\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "        fig, ax = plt.subplots(figsize=(14,10))\n",
    "        means.plot(kind='bar', yerr=stds, capsize=4, color=colors, ax=ax)\n",
    "        ax.set_title(\"Mean values by Sample Type\")\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylabel(\"Mean Value (log)\")\n",
    "        ax.legend(title=\"Parameter\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    elif option == \"Option 2: Visualizing spectral fingerprints according to sample types\":\n",
    "        colors = {\"Clean Water\":\"blue\",\"Contaminated Water\":\"red\",\"Treated Water\":\"green\"}\n",
    "        fig, ax = plt.subplots(figsize=(16,12))\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "            data = group.drop(columns=['Group','Type']).values\n",
    "            for row in data:\n",
    "                ax.plot(wavelengths_values, row, color=colors[sample_type], alpha=0.2)\n",
    "            mean = data.mean(axis=0); std = data.std(axis=0)\n",
    "            ax.plot(wavelengths_values, mean, color=colors[sample_type], linewidth=2, label=f\"{sample_type} mean\")\n",
    "            ax.fill_between(wavelengths_values, mean-std, mean+std, color=colors[sample_type], alpha=0.2)\n",
    "        ax.set_title(\"Spectra by Sample Type\")\n",
    "        ax.set_xlabel(\"Wavelength (nm)\"); ax.set_ylabel(\"Absorbance (AU)\")\n",
    "        ax.legend()\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    elif option == \"Option 3: Separate sample types and their absorbance plots-Interactive!\":\n",
    "        colors = {\"Clean Water\":\"blue\",\"Contaminated Water\":\"red\",\"Treated Water\":\"green\"}\n",
    "        traces, trace_groups = [], {}\n",
    "        for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "            data = group.drop(columns=['Group','Type']).values\n",
    "            mean = data.mean(axis=0); std = data.std(axis=0)\n",
    "            trace_groups[sample_type] = []\n",
    "            for row in data:\n",
    "                idx = len(traces)\n",
    "                traces.append(go.Scatter(x=wavelengths_values,y=row,mode='lines',\n",
    "                                         line=dict(color=colors[sample_type],width=1),\n",
    "                                         opacity=0.2,visible=(sample_type==\"Clean Water\"),\n",
    "                                         name=f\"{sample_type} sample\"))\n",
    "                trace_groups[sample_type].append(idx)\n",
    "            idx = len(traces)\n",
    "            traces.append(go.Scatter(x=wavelengths_values,y=mean,mode='lines',\n",
    "                                     line=dict(color=colors[sample_type],width=3),\n",
    "                                     visible=(sample_type==\"Clean Water\"),\n",
    "                                     name=f\"{sample_type} mean\"))\n",
    "            trace_groups[sample_type].append(idx)\n",
    "        buttons = []\n",
    "        for sample_type in trace_groups:\n",
    "            visible_mask = [False]*len(traces)\n",
    "            for idx in trace_groups[sample_type]: visible_mask[idx]=True\n",
    "            buttons.append(dict(label=sample_type,method=\"update\",\n",
    "                                args=[{\"visible\":visible_mask},{\"title\":f\"{sample_type} Spectra\"}]))\n",
    "        fig = go.Figure(data=traces)\n",
    "        fig.update_layout(title=\"Spectra by Sample Type\",xaxis_title=\"Wavelength (nm)\",\n",
    "                          yaxis_title=\"Absorbance (AU)\",template=\"plotly_white\",\n",
    "                          height=900,width=1200,updatemenus=[dict(active=0,buttons=buttons,x=1,y=1.15,\n",
    "                          xanchor=\"right\",yanchor=\"top\")])\n",
    "        st.plotly_chart(fig)\n",
    "\n",
    "    elif option == \"Option 4: Common peaks and valleys\":\n",
    "        st.write(\"Placeholder text: Why identifying common peaks and valleys is important.\")\n",
    "        colors = {\"Clean Water\":\"blue\",\"Contaminated Water\":\"red\",\"Treated Water\":\"green\"}\n",
    "        mean_spectra,std_spectra={},{}\n",
    "        for sample_type, group in tw_bw_clean_waves.groupby('Type'):\n",
    "            data = group.drop(columns=['Group','Type']).values\n",
    "            mean_spectra[sample_type]=data.mean(axis=0); std_spectra[sample_type]=data.std(axis=0)\n",
    "        maxima_sets,minima_sets=[],[]\n",
    "        for sample_type, mean in mean_spectra.items():\n",
    "            maxima_idx,_=find_peaks(mean); minima_idx,_=find_peaks(-mean)\n",
    "            maxima_sets.append(set(maxima_idx)); minima_sets.append(set(minima_idx))\n",
    "        def find_common_peaks(sets,tolerance=5):\n",
    "            common=[]; ref=sets[0]\n",
    "            for idx in ref:\n",
    "                if all(any(abs(idx-other)<=tolerance for other in s) for s in sets[1:]): common.append(idx)\n",
    "            return np.array(common)\n",
    "        common_maxima=find_common_peaks(maxima_sets); common_minima=find_common_peaks(minima_sets)\n",
    "        fig,ax=plt.subplots(figsize=(16,12))\n",
    "        for sample_type in mean_spectra:\n",
    "            mean=mean_spectra[sample_type]; std=std_spectra[sample_type]\n",
    "            ax.plot(wavelengths_values,mean,color=colors[sample_type],linewidth=2,label=f\"{sample_type} mean\")\n",
    "            ax.fill_between(wavelengths_values,mean-std,mean+std,color=colors[sample_type],alpha=0.2)\n",
    "\n",
    "        \n",
    "        # Continue Option 4 plotting\n",
    "        ax.scatter(wavelengths_values[list(common_maxima)],\n",
    "                   [mean_spectra[t][list(common_maxima)] for t in mean_spectra][0],\n",
    "                   marker='x', s=120, color='black', linewidths=3,\n",
    "                   label=\"Common maxima\")\n",
    "\n",
    "        ax.scatter(wavelengths_values[list(common_minima)],\n",
    "                   [mean_spectra[t][list(common_minima)] for t in mean_spectra][0],\n",
    "                   marker='x', s=120, color='purple', linewidths=3,\n",
    "                   label=\"Common minima\")\n",
    "\n",
    "        ax.set_title(\"Spectra with Common Maxima and Minima\", fontsize=16, fontweight='bold')\n",
    "        ax.set_xlabel(\"Wavelength (nm)\", fontsize=14)\n",
    "        ax.set_ylabel(\"Absorbance (AU)\", fontsize=14)\n",
    "        ax.legend(fontsize=12, loc=\"upper right\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # --- Option 5 ---\n",
    "    elif option == \"Option 5: Summary statistics for common peaks and valleys\":\n",
    "        st.subheader(\"üìë Summary Statistics for Common Peaks and Valleys\")\n",
    "        st.write(\"Below is the summary DataFrame of common peaks and valleys across sample types.\")\n",
    "        st.dataframe(compeak_summary)\n",
    "\n",
    "    # --- Option 6 ---\n",
    "    elif option == \"Option 6: Correlation? You decide\":\n",
    "        st.subheader(\"üîó Correlation Analysis\")\n",
    "        st.write(\"Heatmap showing correlation of absorbance at common wavelengths with TS, TSS, and Turbidity.\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(18,12))  # larger heatmap\n",
    "        sns.heatmap(\n",
    "            physio_corr,\n",
    "            cmap=\"coolwarm\",\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cbar_kws={'label': 'Correlation coefficient'},\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(\"Correlation of Absorbance at Common Wavelengths with TS, TSS, and Turbidity\",\n",
    "                     fontsize=18, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Wavelengths\", fontsize=14)\n",
    "        ax.set_ylabel(\"Physiochemical Parameters\", fontsize=14)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "\n",
    "def page4():\n",
    "    st.title(\"Principal Component Analysis ‚Äì Are my samples discernibly different? üîç\")\n",
    "\n",
    "    # Toggle between PCA biplot and Scree plot\n",
    "    view_option = st.radio(\n",
    "        \"Select plot to display:\",\n",
    "        [\"PCA Biplot\", \"Scree Plot\"],\n",
    "        horizontal=True\n",
    "    )\n",
    "\n",
    "    # --- PCA Biplot ---\n",
    "    if view_option == \"PCA Biplot\":\n",
    "        st.markdown(\n",
    "            \"<p style='color:black; font-size:18px; font-weight:bold;'>\"\n",
    "            \"Placeholder: Explain PCA separation of sample types and how loadings arrows show feature contributions.\"\n",
    "            \"</p>\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Color-coded PCA (red, green, blue)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pca_df[\"PC1\"], y=pca_df[\"PC2\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=12),\n",
    "            text=pca_df[\"Type\"],\n",
    "            name=\"Color-coded PCA\",\n",
    "            marker_color=pca_df[\"Type\"].map({\n",
    "                \"Clean Water\": \"blue\",\n",
    "                \"Dirty Water\": \"red\",\n",
    "                \"Treated Water\": \"green\"\n",
    "            })\n",
    "        ))\n",
    "\n",
    "        # Uncoded PCA (all gray)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=pca_df[\"PC1\"], y=pca_df[\"PC2\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=12, color=\"gray\"),\n",
    "            text=pca_df[\"Type\"],\n",
    "            name=\"Uncoded PCA\"\n",
    "        ))\n",
    "\n",
    "        # Variance directional arrows (loadings for PC1 and PC2 only)\n",
    "        loadings = pca.components_.T[:, :2]\n",
    "        for i, feature in enumerate(X.columns):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[0, loadings[i, 0] * 3],\n",
    "                y=[0, loadings[i, 1] * 3],\n",
    "                mode=\"lines+text\",\n",
    "                line=dict(color=\"black\"),\n",
    "                text=[None, feature],\n",
    "                textposition=\"top center\",\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "        # Dropdown menu at top middle\n",
    "        fig.update_layout(\n",
    "            updatemenus=[\n",
    "                dict(\n",
    "                    buttons=list([\n",
    "                        dict(label=\"Color-coded PCA\",\n",
    "                             method=\"update\",\n",
    "                             args=[{\"visible\": [True, False] + [True] * len(X.columns)},\n",
    "                                   {\"title\": \"PCA Biplot (Color-coded by Sample Type)\"}]),\n",
    "                        dict(label=\"Uncoded PCA\",\n",
    "                             method=\"update\",\n",
    "                             args=[{\"visible\": [False, True] + [True] * len(X.columns)},\n",
    "                                   {\"title\": \"PCA Biplot (Uncoded)\"}])\n",
    "                    ]),\n",
    "                    direction=\"down\",\n",
    "                    showactive=True,\n",
    "                    x=0.5, y=1.15,   # üîë top middle\n",
    "                    xanchor=\"center\", yanchor=\"top\"\n",
    "                )\n",
    "            ],\n",
    "            title=\"PCA Biplot\",\n",
    "            xaxis_title=f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)\",\n",
    "            yaxis_title=f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)\",\n",
    "            height=1000, width=1200,\n",
    "            plot_bgcolor=\"white\", paper_bgcolor=\"white\"\n",
    "        )\n",
    "\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # --- Scree Plot ---\n",
    "\n",
    "    elif view_option == \"Scree Plot\":\n",
    "        st.markdown(\n",
    "            \"<p style='color:black; font-size:18px; font-weight:bold;'>\"\n",
    "            \"Placeholder: Explain how much variance each principal component explains (scree plot).\"\n",
    "            \"</p>\",\n",
    "            unsafe_allow_html=True\n",
    "        )\n",
    "    \n",
    "        # Scree plot of explained variance (scatter + line)\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[f\"PC{i+1}\" for i in range(pca.n_components_)],\n",
    "            y=pca.explained_variance_ratio_ * 100,\n",
    "            mode=\"lines+markers\",   # üîë scatter with lines\n",
    "            marker=dict(size=10, color=\"purple\"),\n",
    "            line=dict(color=\"purple\", width=2),\n",
    "            name=\"Variance Explained\"\n",
    "        ))\n",
    "    \n",
    "        fig.update_layout(\n",
    "            title=\"Scree Plot ‚Äì Variance Explained by Principal Components\",\n",
    "            xaxis_title=\"Principal Components\",\n",
    "            yaxis_title=\"Variance Explained (%)\",\n",
    "            height=900, width=1200,\n",
    "            plot_bgcolor=\"white\", paper_bgcolor=\"white\"\n",
    "        )\n",
    "    \n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main App Navigation\n",
    "# -------------------------\n",
    "st.sidebar.title(\"Navigation\")\n",
    "page = st.sidebar.radio(\"Go to:\", [\n",
    "    \"Page 1: Project Overview\",\n",
    "    \"Page 2: IDA\",\n",
    "    \"Page 3: EDA: Exploratory Data Analysis\",\n",
    "    \"Page 4: PCA ‚Äì Are my samples discernibly different?\"\n",
    "])\n",
    "\n",
    "if page == \"Page 1: Project Overview\":\n",
    "    page1()\n",
    "elif page == \"Page 2: IDA\":\n",
    "    page2()\n",
    "elif page == \"Page 3: EDA: Exploratory Data Analysis\":\n",
    "    page3()\n",
    "elif page == \"Page 4: PCA ‚Äì Are my samples discernibly different?\":\n",
    "    page4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d43cd-4331-415f-99ff-79290ac171c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74a921-39cd-483f-9735-3a468d61ecad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
